{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis - Gaia DR3 Stellar Data\n",
    "\n",
    "This notebook performs initial exploration of the Gaia DR3 dataset to understand the stellar population before applying anomaly detection methods.\n",
    "\n",
    "## Objectives:\n",
    "1. Load and examine the processed Gaia DR3 data\n",
    "2. Create Hertzsprung-Russell diagrams\n",
    "3. Analyze proper motion distributions\n",
    "4. Identify known stellar populations\n",
    "5. Prepare data for anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Import our custom modules\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from visualization import StellarVisualization\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processed Gaia DR3 data\n",
    "data_file = \"../data/gaia_dr3_processed.csv\"\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(data_file)\n",
    "    print(f\"Loaded {len(df):,} stellar sources\")\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"\\nColumns: {list(df.columns)}\")\nexcept FileNotFoundError:\n",
    "    print(f\"Error: {data_file} not found.\")\n",
    "    print(\"Please run '../src/data_acquisition.py' first to download the data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data info\n",
    "if 'df' in locals():\n",
    "    print(\"Dataset Info:\")\n",
    "    print(df.info())\n",
    "    \n",
    "    print(\"\\nBasic Statistics:\")\n",
    "    print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "if 'df' in locals():\n",
    "    missing_values = df.isnull().sum()\n",
    "    if missing_values.sum() > 0:\n",
    "        print(\"Missing values per column:\")\n",
    "        for col, count in missing_values.items():\n",
    "            if count > 0:\n",
    "                print(f\"  {col}: {count} ({count/len(df)*100:.1f}%)\")\n",
    "    else:\n",
    "        print(\"No missing values found!\")\n",
    "    \n",
    "    # Check data ranges\n",
    "    print(\"\\nData Ranges:\")\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        print(f\"  {col}: [{df[col].min():.3f}, {df[col].max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hertzsprung-Russell Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HR diagram\n",
    "if 'df' in locals():\n",
    "    viz = StellarVisualization()\n",
    "    \n",
    "    # Static HR diagram\n",
    "    fig, ax = viz.plot_hr_diagram(\n",
    "        df, \n",
    "        color_col='bp_rp', \n",
    "        mag_col='abs_g_mag',\n",
    "        title=\"Gaia DR3 Hertzsprung-Russell Diagram\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive HR diagram\n",
    "if 'df' in locals():\n",
    "    interactive_fig = viz.create_interactive_hr_diagram(\n",
    "        df, \n",
    "        save_html=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Proper Motion Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proper motion analysis\n",
    "if 'df' in locals():\n",
    "    fig = viz.plot_proper_motion_distribution(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature correlation analysis\n",
    "if 'df' in locals():\n",
    "    fig, corr_matrix = viz.plot_feature_correlations(df)\n",
    "    \n",
    "    # Print highly correlated features\n",
    "    print(\"\\nHighly Correlated Feature Pairs (|r| > 0.7):\")\n",
    "    high_corr = []\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            corr_val = corr_matrix.iloc[i, j]\n",
    "            if abs(corr_val) > 0.7:\n",
    "                high_corr.append((\n",
    "                    corr_matrix.columns[i], \n",
    "                    corr_matrix.columns[j], \n",
    "                    corr_val\n",
    "                ))\n",
    "    \n",
    "    for feat1, feat2, corr in sorted(high_corr, key=lambda x: abs(x[2]), reverse=True):\n",
    "        print(f\"  {feat1} <-> {feat2}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Identify Known Stellar Populations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify different stellar populations based on HR diagram position\n",
    "if 'df' in locals():\n",
    "    print(\"Stellar Population Analysis:\")\n",
    "    \n",
    "    # Main sequence stars (rough criteria)\n",
    "    main_sequence = df[\n",
    "        (df['bp_rp'] > -0.5) & (df['bp_rp'] < 2.0) &\n",
    "        (df['abs_g_mag'] > (4.83 + 5.5 * df['bp_rp'] - 2)) &\n",
    "        (df['abs_g_mag'] < (4.83 + 5.5 * df['bp_rp'] + 2))\n",
    "    ]\n",
    "    print(f\"  Main Sequence Stars: {len(main_sequence):,} ({len(main_sequence)/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # White dwarfs (hot and faint)\n",
    "    white_dwarfs = df[\n",
    "        (df['bp_rp'] < 0.5) & (df['abs_g_mag'] > 10)\n",
    "    ]\n",
    "    print(f\"  White Dwarf Candidates: {len(white_dwarfs):,} ({len(white_dwarfs)/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Red giants (cool and bright)\n",
    "    red_giants = df[\n",
    "        (df['bp_rp'] > 1.0) & (df['abs_g_mag'] < 4)\n",
    "    ]\n",
    "    print(f\"  Red Giant Candidates: {len(red_giants):,} ({len(red_giants)/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # High proper motion stars (potential nearby objects)\n",
    "    if 'pm_total' in df.columns:\n",
    "        high_pm = df[df['pm_total'] > 50]  # > 50 mas/yr\n",
    "        print(f\"  High Proper Motion Stars (>50 mas/yr): {len(high_pm):,} ({len(high_pm)/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # High velocity stars (potential hypervelocity candidates)\n",
    "    if 'v_tan_km_s' in df.columns:\n",
    "        high_velocity = df[df['v_tan_km_s'] > 100]  # > 100 km/s\n",
    "        print(f\"  High Velocity Stars (>100 km/s): {len(high_velocity):,} ({len(high_velocity)/len(df)*100:.1f}%)\")\n",
    "        \n",
    "        # Extreme velocity candidates\n",
    "        extreme_velocity = df[df['v_tan_km_s'] > 300]  # > 300 km/s\n",
    "        print(f\"  Extreme Velocity Candidates (>300 km/s): {len(extreme_velocity):,}\")\n",
    "        \n",
    "        if len(extreme_velocity) > 0:\n",
    "            print(\"\\n  Top 5 Highest Velocity Objects:\")\n",
    "            top_velocity = extreme_velocity.nlargest(5, 'v_tan_km_s')\n",
    "            for idx, row in top_velocity.iterrows():\n",
    "                print(f\"    Source {row['source_id']}: {row['v_tan_km_s']:.1f} km/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Summary for Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary for anomaly detection preparation\n",
    "if 'df' in locals():\n",
    "    print(\"=== Data Summary for Anomaly Detection ===\")\n",
    "    print(f\"Total sources: {len(df):,}\")\n",
    "    print(f\"Features available: {len(df.columns)}\")\n",
    "    \n",
    "    # Key features for ML\n",
    "    ml_features = [\n",
    "        'parallax', 'pmra', 'pmdec', 'pm_total',\n",
    "        'phot_g_mean_mag', 'abs_g_mag', 'bp_rp',\n",
    "        'distance_pc', 'v_tan_km_s', 'reduced_pm', 'ruwe'\n",
    "    ]\n",
    "    \n",
    "    available_ml_features = [f for f in ml_features if f in df.columns]\n",
    "    print(f\"ML-ready features: {len(available_ml_features)}\")\n",
    "    print(f\"Features: {available_ml_features}\")\n",
    "    \n",
    "    # Data quality for ML\n",
    "    ml_df = df[available_ml_features]\n",
    "    complete_cases = ml_df.dropna()\n",
    "    print(f\"\\nComplete cases for ML: {len(complete_cases):,} ({len(complete_cases)/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    print(\"\\nReady for preprocessing and anomaly detection!\")\n",
    "    print(\"Next steps:\")\n",
    "    print(\"1. Run ../src/preprocessing.py to prepare training data\")\n",
    "    print(\"2. Run ../src/models.py to train anomaly detection models\")\n",
    "    print(\"3. Analyze results and create visualizations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "This exploratory analysis provides a foundation for anomaly detection by:\n",
    "\n",
    "1. **Data Quality**: Confirming the dataset is clean and suitable for ML\n",
    "2. **Stellar Populations**: Identifying known populations that should be \"normal\"\n",
    "3. **Feature Understanding**: Understanding correlations and distributions\n",
    "4. **Baseline Expectations**: Setting expectations for what constitutes an anomaly\n",
    "\n",
    "The next phase will involve training unsupervised ML models to identify objects that deviate significantly from these known populations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
